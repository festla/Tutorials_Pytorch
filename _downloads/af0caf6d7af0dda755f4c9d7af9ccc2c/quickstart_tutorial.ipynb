{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/festla/Tutorials_Pytorch/blob/main/_downloads/af0caf6d7af0dda755f4c9d7af9ccc2c/quickstart_tutorial.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "scWIGXaJ_8QX"
      },
      "outputs": [],
      "source": [
        "# For tips on running notebooks in Google Colab, see\n",
        "# https://pytorch.org/tutorials/beginner/colab\n",
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hh0npxS7_8QZ"
      },
      "source": [
        "[Learn the Basics](intro.html) \\|\\| **Quickstart** \\|\\|\n",
        "[Tensors](tensorqs_tutorial.html) \\|\\| [Datasets &\n",
        "DataLoaders](data_tutorial.html) \\|\\|\n",
        "[Transforms](transforms_tutorial.html) \\|\\| [Build\n",
        "Model](buildmodel_tutorial.html) \\|\\|\n",
        "[Autograd](autogradqs_tutorial.html) \\|\\|\n",
        "[Optimization](optimization_tutorial.html) \\|\\| [Save & Load\n",
        "Model](saveloadrun_tutorial.html)\n",
        "\n",
        "Quickstart\n",
        "==========\n",
        "\n",
        "This section runs through the API for common tasks in machine learning.\n",
        "Refer to the links in each section to dive deeper.\n",
        "\n",
        "Working with data\n",
        "-----------------\n",
        "\n",
        "PyTorch has two [primitives to work with\n",
        "data](https://pytorch.org/docs/stable/data.html):\n",
        "`torch.utils.data.DataLoader` and `torch.utils.data.Dataset`. `Dataset`\n",
        "stores the samples and their corresponding labels, and `DataLoader`\n",
        "wraps an iterable around the `Dataset`.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "N7sTVRNo_8QZ"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision import datasets\n",
        "from torchvision.transforms import ToTensor"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NTu8MD5q_8Qa"
      },
      "source": [
        "PyTorch offers domain-specific libraries such as\n",
        "[TorchText](https://pytorch.org/text/stable/index.html),\n",
        "[TorchVision](https://pytorch.org/vision/stable/index.html), and\n",
        "[TorchAudio](https://pytorch.org/audio/stable/index.html), all of which\n",
        "include datasets. For this tutorial, we will be using a TorchVision\n",
        "dataset.\n",
        "\n",
        "The `torchvision.datasets` module contains `Dataset` objects for many\n",
        "real-world vision data like CIFAR, COCO ([full list\n",
        "here](https://pytorch.org/vision/stable/datasets.html)). In this\n",
        "tutorial, we use the FashionMNIST dataset. Every TorchVision `Dataset`\n",
        "includes two arguments: `transform` and `target_transform` to modify the\n",
        "samples and labels respectively.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tOzvIlCK_8Qa",
        "outputId": "d33b9148-5c7c-49a9-d92a-164c6d2de407"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 26.4M/26.4M [00:02<00:00, 13.0MB/s]\n",
            "100%|██████████| 29.5k/29.5k [00:00<00:00, 207kB/s]\n",
            "100%|██████████| 4.42M/4.42M [00:01<00:00, 3.93MB/s]\n",
            "100%|██████████| 5.15k/5.15k [00:00<00:00, 16.7MB/s]\n"
          ]
        }
      ],
      "source": [
        "# Download training data from open datasets.\n",
        "training_data = datasets.FashionMNIST(\n",
        "    root=\"data\",\n",
        "    train=True,\n",
        "    download=True,\n",
        "    transform=ToTensor(),\n",
        ")\n",
        "\n",
        "# Download test data from open datasets.\n",
        "test_data = datasets.FashionMNIST(\n",
        "    root=\"data\",\n",
        "    train=False,\n",
        "    download=True,\n",
        "    transform=ToTensor(),\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2hJEnNXx_8Qa"
      },
      "source": [
        "We pass the `Dataset` as an argument to `DataLoader`. This wraps an\n",
        "iterable over our dataset, and supports automatic batching, sampling,\n",
        "shuffling and multiprocess data loading. Here we define a batch size of\n",
        "64, i.e. each element in the dataloader iterable will return a batch of\n",
        "64 features and labels.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5DzPlyxa_8Qa",
        "outputId": "93a61176-5e78-47d7-a8f4-94fdaacdeed9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Shape of X [N, C, H, W]: torch.Size([64, 1, 28, 28])\n",
            "Shape of y: torch.Size([64]) torch.int64\n"
          ]
        }
      ],
      "source": [
        "batch_size = 64\n",
        "\n",
        "# Create data loaders.\n",
        "train_dataloader = DataLoader(training_data, batch_size=batch_size)\n",
        "test_dataloader = DataLoader(test_data, batch_size=batch_size)\n",
        "\n",
        "for X, y in test_dataloader:\n",
        "    print(f\"Shape of X [N, C, H, W]: {X.shape}\")\n",
        "    print(f\"Shape of y: {y.shape} {y.dtype}\")\n",
        "    break"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IUE9pHwI_8Qa"
      },
      "source": [
        "Read more about [loading data in PyTorch](data_tutorial.html).\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dp1LRfhQ_8Qa"
      },
      "source": [
        "------------------------------------------------------------------------\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eNVR8AxC_8Qb"
      },
      "source": [
        "Creating Models\n",
        "===============\n",
        "\n",
        "To define a neural network in PyTorch, we create a class that inherits\n",
        "from\n",
        "[nn.Module](https://pytorch.org/docs/stable/generated/torch.nn.Module.html).\n",
        "We define the layers of the network in the `__init__` function and\n",
        "specify how data will pass through the network in the `forward`\n",
        "function. To accelerate operations in the neural network, we move it to\n",
        "the\n",
        "[accelerator](https://pytorch.org/docs/stable/torch.html#accelerators)\n",
        "such as CUDA, MPS, MTIA, or XPU. If the current accelerator is\n",
        "available, we will use it. Otherwise, we use the CPU.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i5xhWfyC_8Qb",
        "outputId": "a14f02e0-7490-4562-d8d4-7269b0c34824"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using cuda device\n",
            "NeuralNetwork(\n",
            "  (flatten): Flatten(start_dim=1, end_dim=-1)\n",
            "  (linear_relu_stack): Sequential(\n",
            "    (0): Linear(in_features=784, out_features=512, bias=True)\n",
            "    (1): ReLU()\n",
            "    (2): Linear(in_features=512, out_features=512, bias=True)\n",
            "    (3): ReLU()\n",
            "    (4): Linear(in_features=512, out_features=10, bias=True)\n",
            "  )\n",
            ")\n"
          ]
        }
      ],
      "source": [
        "device = torch.accelerator.current_accelerator().type if torch.accelerator.is_available() else \"cpu\"\n",
        "print(f\"Using {device} device\")\n",
        "\n",
        "# Define model\n",
        "class NeuralNetwork(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.flatten = nn.Flatten()\n",
        "        self.linear_relu_stack = nn.Sequential(\n",
        "            nn.Linear(28*28, 512),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(512, 512),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(512, 10)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.flatten(x)\n",
        "        logits = self.linear_relu_stack(x)\n",
        "        return logits\n",
        "\n",
        "model = NeuralNetwork().to(device)\n",
        "print(model)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3Eke8g2B_8Qb"
      },
      "source": [
        "Read more about [building neural networks in\n",
        "PyTorch](buildmodel_tutorial.html).\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P5EKLr7c_8Qb"
      },
      "source": [
        "------------------------------------------------------------------------\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PVpFNgj__8Qb"
      },
      "source": [
        "Optimizing the Model Parameters\n",
        "===============================\n",
        "\n",
        "To train a model, we need a [loss\n",
        "function](https://pytorch.org/docs/stable/nn.html#loss-functions) and an\n",
        "[optimizer](https://pytorch.org/docs/stable/optim.html).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "RPVZG4QA_8Qb"
      },
      "outputs": [],
      "source": [
        "loss_fn = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=1e-3)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5Poum3AC_8Qb"
      },
      "source": [
        "In a single training loop, the model makes predictions on the training\n",
        "dataset (fed to it in batches), and backpropagates the prediction error\n",
        "to adjust the model\\'s parameters.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "_FLzqtrp_8Qb"
      },
      "outputs": [],
      "source": [
        "def train(dataloader, model, loss_fn, optimizer):\n",
        "    size = len(dataloader.dataset)\n",
        "    model.train()\n",
        "    for batch_idx, (X, y) in enumerate(dataloader):\n",
        "        X, y = X.to(device), y.to(device)\n",
        "\n",
        "        # Compute prediction error\n",
        "        pred = model(X)\n",
        "        loss = loss_fn(pred, y)\n",
        "        # Backpropagation\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        if batch_idx % 100 == 0:\n",
        "            loss, current = loss.item(), (batch_idx + 1) * len(X)\n",
        "            print(f\"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yzuVBkzB_8Qb"
      },
      "source": [
        "We also check the model\\'s performance against the test dataset to\n",
        "ensure it is learning.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "tKxcmK76_8Qb"
      },
      "outputs": [],
      "source": [
        "def test(dataloader, model, loss_fn):\n",
        "    size = len(dataloader.dataset)\n",
        "    num_batches = len(dataloader)\n",
        "    model.eval()\n",
        "    test_loss, correct = 0, 0\n",
        "    with torch.no_grad():\n",
        "        for X, y in dataloader:\n",
        "            X, y = X.to(device), y.to(device)\n",
        "            pred = model(X)\n",
        "            test_loss += loss_fn(pred, y).item()    # .item() 将张量中的标量值提取为 Python 数值（避免内存占用）。\n",
        "            correct += (pred.argmax(1) == y).type(torch.float).sum().item()\n",
        "    test_loss /= num_batches\n",
        "    correct /= size\n",
        "    print(f\"Test Error: \\n Accuracy: {(100*correct):>0.1f}%, Avg loss: {test_loss:>8f} \\n\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8q4XPL6G_8Qb"
      },
      "source": [
        "The training process is conducted over several iterations (*epochs*).\n",
        "During each epoch, the model learns parameters to make better\n",
        "predictions. We print the model\\'s accuracy and loss at each epoch;\n",
        "we\\'d like to see the accuracy increase and the loss decrease with every\n",
        "epoch.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C7DvwpPX_8Qb",
        "outputId": "a9f66bf8-c3d8-4503-a3e5-e0f0e2f1affb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1\n",
            "-------------------------------\n",
            "loss: 1.140117  [   64/60000]\n",
            "loss: 1.140074  [ 6464/60000]\n",
            "loss: 0.962849  [12864/60000]\n",
            "loss: 1.093417  [19264/60000]\n",
            "loss: 0.981574  [25664/60000]\n",
            "loss: 1.005439  [32064/60000]\n",
            "loss: 1.042871  [38464/60000]\n",
            "loss: 0.973481  [44864/60000]\n",
            "loss: 1.026849  [51264/60000]\n",
            "loss: 0.956810  [57664/60000]\n",
            "Test Error: \n",
            " Accuracy: 66.0%, Avg loss: 0.965316 \n",
            "\n",
            "Epoch 2\n",
            "-------------------------------\n",
            "loss: 1.021647  [   64/60000]\n",
            "loss: 1.040273  [ 6464/60000]\n",
            "loss: 0.848262  [12864/60000]\n",
            "loss: 1.002175  [19264/60000]\n",
            "loss: 0.897344  [25664/60000]\n",
            "loss: 0.912745  [32064/60000]\n",
            "loss: 0.965270  [38464/60000]\n",
            "loss: 0.901168  [44864/60000]\n",
            "loss: 0.951501  [51264/60000]\n",
            "loss: 0.891320  [57664/60000]\n",
            "Test Error: \n",
            " Accuracy: 67.3%, Avg loss: 0.894462 \n",
            "\n",
            "Epoch 3\n",
            "-------------------------------\n",
            "loss: 0.936668  [   64/60000]\n",
            "loss: 0.972619  [ 6464/60000]\n",
            "loss: 0.768317  [12864/60000]\n",
            "loss: 0.939150  [19264/60000]\n",
            "loss: 0.842313  [25664/60000]\n",
            "loss: 0.846255  [32064/60000]\n",
            "loss: 0.911007  [38464/60000]\n",
            "loss: 0.854273  [44864/60000]\n",
            "loss: 0.898592  [51264/60000]\n",
            "loss: 0.845374  [57664/60000]\n",
            "Test Error: \n",
            " Accuracy: 68.7%, Avg loss: 0.844268 \n",
            "\n",
            "Epoch 4\n",
            "-------------------------------\n",
            "loss: 0.872354  [   64/60000]\n",
            "loss: 0.923251  [ 6464/60000]\n",
            "loss: 0.709298  [12864/60000]\n",
            "loss: 0.893477  [19264/60000]\n",
            "loss: 0.803553  [25664/60000]\n",
            "loss: 0.796552  [32064/60000]\n",
            "loss: 0.870325  [38464/60000]\n",
            "loss: 0.822397  [44864/60000]\n",
            "loss: 0.859475  [51264/60000]\n",
            "loss: 0.810564  [57664/60000]\n",
            "Test Error: \n",
            " Accuracy: 69.8%, Avg loss: 0.806596 \n",
            "\n",
            "Epoch 5\n",
            "-------------------------------\n",
            "loss: 0.821324  [   64/60000]\n",
            "loss: 0.884328  [ 6464/60000]\n",
            "loss: 0.663539  [12864/60000]\n",
            "loss: 0.858806  [19264/60000]\n",
            "loss: 0.773976  [25664/60000]\n",
            "loss: 0.758261  [32064/60000]\n",
            "loss: 0.837723  [38464/60000]\n",
            "loss: 0.799135  [44864/60000]\n",
            "loss: 0.829304  [51264/60000]\n",
            "loss: 0.782737  [57664/60000]\n",
            "Test Error: \n",
            " Accuracy: 71.0%, Avg loss: 0.776732 \n",
            "\n",
            "Done!\n"
          ]
        }
      ],
      "source": [
        "epochs = 5\n",
        "for t in range(epochs):\n",
        "    print(f\"Epoch {t+1}\\n-------------------------------\")\n",
        "    train(train_dataloader, model, loss_fn, optimizer)\n",
        "    test(test_dataloader, model, loss_fn)\n",
        "print(\"Done!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9AbUh1D2_8Qb"
      },
      "source": [
        "Read more about [Training your model](optimization_tutorial.html).\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "print(np.__version__)  # 例如输出：2.0.1"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YUMXEKPcJAMP",
        "outputId": "6d760776-08aa-4a4b-e4ca-9c6831931b6a"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2.0.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c0u9FEoY_8Qb"
      },
      "source": [
        "------------------------------------------------------------------------\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Et1nus8-_8Qb"
      },
      "source": [
        "Saving Models\n",
        "=============\n",
        "\n",
        "A common way to save a model is to serialize the internal state\n",
        "dictionary (containing the model parameters).\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.state_dict()"
      ],
      "metadata": {
        "collapsed": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6iBg8YWyJmgV",
        "outputId": "c70911aa-ae75-4d75-a1fa-735ae8a8e123"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "OrderedDict([('linear_relu_stack.0.weight',\n",
              "              tensor([[ 0.0108, -0.0301, -0.0281,  ...,  0.0056, -0.0119, -0.0333],\n",
              "                      [ 0.0216,  0.0260,  0.0026,  ..., -0.0227, -0.0253, -0.0347],\n",
              "                      [ 0.0297, -0.0162, -0.0128,  ...,  0.0202, -0.0193,  0.0042],\n",
              "                      ...,\n",
              "                      [-0.0048, -0.0261, -0.0345,  ...,  0.0255, -0.0285,  0.0128],\n",
              "                      [-0.0171, -0.0099, -0.0110,  ..., -0.0143, -0.0062, -0.0242],\n",
              "                      [-0.0186,  0.0221, -0.0318,  ...,  0.0080,  0.0170, -0.0102]],\n",
              "                     device='cuda:0')),\n",
              "             ('linear_relu_stack.0.bias',\n",
              "              tensor([ 2.4937e-02,  2.5753e-02, -2.2636e-02,  1.1718e-02,  5.6196e-03,\n",
              "                       1.6040e-02,  1.0593e-02,  1.3490e-02,  1.0594e-02,  1.9933e-02,\n",
              "                       2.1735e-02,  3.1874e-02,  3.2512e-02,  2.7233e-02, -7.6174e-03,\n",
              "                       3.8684e-02, -1.9537e-02,  1.3597e-02,  2.1942e-02,  1.2294e-02,\n",
              "                      -7.7241e-04, -3.4299e-02,  2.1470e-02, -3.1481e-02,  8.9310e-03,\n",
              "                       3.9075e-02,  1.0554e-02,  2.1536e-02,  8.7141e-04,  1.6745e-02,\n",
              "                       2.1965e-02,  3.6412e-02,  2.5100e-02,  2.5329e-02,  2.8897e-03,\n",
              "                       2.9627e-02,  6.4901e-03,  8.9227e-03, -1.9934e-02,  4.5007e-02,\n",
              "                       3.9803e-02, -1.9904e-02,  2.8201e-02,  1.8748e-02, -8.1067e-03,\n",
              "                       2.1687e-02, -2.8474e-02,  6.6189e-03, -2.2957e-02,  2.1405e-02,\n",
              "                      -7.9804e-03, -6.0367e-04,  2.6495e-03, -2.5082e-02,  4.3316e-03,\n",
              "                      -1.8972e-02, -2.2774e-02, -1.7221e-02, -1.5656e-02,  2.8206e-02,\n",
              "                      -3.2758e-02,  2.1074e-02,  1.6972e-02,  3.1066e-02,  1.9041e-02,\n",
              "                      -2.3210e-03, -8.5355e-03,  2.7489e-02,  3.5221e-02, -2.3208e-02,\n",
              "                      -1.2176e-02,  2.0675e-02, -1.0328e-03,  2.8589e-03, -3.3636e-02,\n",
              "                       4.5002e-02, -9.4809e-04, -4.0460e-03,  4.4231e-03, -3.0656e-02,\n",
              "                       2.6932e-02, -2.4288e-02, -2.1374e-02, -1.5954e-02,  1.3376e-02,\n",
              "                      -1.7018e-02, -2.7756e-02,  5.4409e-03, -8.0415e-03, -3.3097e-03,\n",
              "                      -1.4128e-02,  4.6343e-03, -2.4383e-02,  2.3143e-02,  3.5623e-03,\n",
              "                       1.1815e-02, -7.6189e-03,  3.0208e-02,  9.5484e-03,  3.4198e-02,\n",
              "                      -1.5980e-02,  2.3555e-02, -1.2200e-02,  1.7141e-02,  1.7028e-02,\n",
              "                       2.9659e-02, -1.0536e-03,  4.0607e-02,  1.6225e-02,  1.4020e-02,\n",
              "                       3.6376e-02,  2.3780e-02, -1.2639e-03,  2.1117e-02,  5.0129e-03,\n",
              "                       2.8814e-02,  3.9540e-02, -2.1523e-02, -2.3799e-02,  7.1615e-03,\n",
              "                       3.4939e-02, -2.6971e-02, -1.4263e-02, -4.0986e-03,  1.0665e-02,\n",
              "                       1.9473e-03, -3.5009e-02,  2.2378e-02, -9.2361e-03, -3.4094e-02,\n",
              "                      -1.0599e-02, -2.8667e-02, -1.9587e-02, -2.4652e-02,  2.3693e-02,\n",
              "                      -2.7675e-02, -1.3927e-03, -2.1768e-02, -5.0084e-03,  1.9691e-02,\n",
              "                       2.6606e-02, -5.1074e-03, -2.1181e-02,  2.9740e-02,  7.1122e-03,\n",
              "                      -3.2616e-02, -1.0857e-02, -1.6604e-02, -3.0739e-02, -3.3395e-02,\n",
              "                      -1.8079e-02, -2.6100e-02, -1.4983e-02, -2.7111e-03,  1.3386e-02,\n",
              "                       3.5403e-02,  2.2750e-02, -1.8023e-02, -2.0267e-02, -2.5729e-02,\n",
              "                      -2.9786e-02, -1.8029e-02, -1.0777e-02,  3.6928e-02,  2.4922e-02,\n",
              "                      -8.0515e-03, -1.1906e-02,  2.9831e-02, -1.3618e-02, -3.5031e-03,\n",
              "                      -1.1843e-02,  3.7020e-02,  1.1987e-02,  2.2625e-02, -1.5223e-02,\n",
              "                       3.2184e-02,  1.4680e-02,  1.1616e-03, -1.4067e-02, -9.7327e-03,\n",
              "                       1.4976e-02, -3.1612e-03,  3.5224e-02,  1.2120e-02,  3.8133e-02,\n",
              "                       1.7979e-02, -2.8811e-02, -8.5564e-03,  3.8562e-02,  4.2306e-02,\n",
              "                      -6.8725e-03,  3.0455e-02, -1.9890e-03,  1.4078e-02,  3.6587e-02,\n",
              "                      -1.1600e-02, -2.0134e-02,  1.5851e-02, -2.5212e-03, -1.7863e-02,\n",
              "                       3.2267e-02,  6.5371e-03,  2.2364e-02, -8.0420e-03,  3.4877e-02,\n",
              "                       1.2902e-02,  3.2722e-03, -9.5749e-03,  2.3880e-02, -7.5037e-03,\n",
              "                       1.7959e-03, -3.2845e-02,  9.2370e-03, -2.6377e-02,  2.5098e-02,\n",
              "                       1.5523e-02,  4.6317e-02,  3.4187e-02,  4.8994e-02,  3.9116e-02,\n",
              "                      -1.2207e-02,  2.4060e-02,  2.5500e-02, -2.0014e-02,  4.3845e-03,\n",
              "                       2.3593e-03,  2.7332e-02,  3.1411e-02, -8.6267e-03, -1.0398e-03,\n",
              "                       1.9745e-02,  3.8593e-02,  3.9098e-02, -8.1836e-04,  2.2520e-02,\n",
              "                      -1.2847e-02,  2.5281e-02,  3.7090e-03, -3.9714e-03, -8.3398e-03,\n",
              "                       4.2623e-02,  3.2957e-02,  1.0556e-02,  4.8598e-02, -2.6285e-02,\n",
              "                       3.8983e-02,  2.1652e-02, -8.7792e-03, -2.6006e-02,  1.2559e-02,\n",
              "                      -7.9015e-03, -7.5826e-03,  1.1042e-04, -1.4617e-02, -5.9088e-03,\n",
              "                      -2.1788e-03, -1.7690e-02,  5.8882e-03, -2.5826e-02,  3.9139e-02,\n",
              "                       4.4943e-03, -1.2642e-02, -2.1497e-02,  3.4388e-02, -2.6998e-02,\n",
              "                       4.5710e-04,  9.1493e-06,  9.8103e-03, -2.6105e-02, -1.6361e-04,\n",
              "                       3.5141e-02,  9.5706e-03,  5.8924e-03, -7.3096e-03, -2.7288e-03,\n",
              "                       1.5564e-02,  2.9352e-02, -1.7592e-02,  8.3363e-03, -3.4229e-02,\n",
              "                       1.0130e-02, -2.9567e-02, -5.2990e-03,  1.9894e-02,  6.2800e-03,\n",
              "                       3.4207e-02,  2.3025e-02,  2.0881e-02,  1.5376e-02,  2.3471e-02,\n",
              "                      -7.1398e-04,  3.0237e-02, -7.1026e-03, -1.5025e-02, -2.2062e-03,\n",
              "                      -6.0666e-03,  7.2269e-03,  9.5266e-03,  5.7529e-03, -2.2198e-02,\n",
              "                      -7.1277e-03,  1.4137e-03, -3.4046e-02, -3.2145e-02,  1.4181e-02,\n",
              "                      -3.5077e-03, -3.6416e-02, -8.2705e-03, -1.3411e-03, -4.8966e-04,\n",
              "                       6.1159e-03,  9.8952e-03,  1.3834e-03,  2.0554e-02, -2.1774e-02,\n",
              "                       6.4262e-03,  1.7994e-02, -1.9843e-02,  1.6745e-02,  3.1513e-02,\n",
              "                      -1.2540e-02,  7.4166e-03, -5.4703e-04,  5.1154e-03, -2.1665e-02,\n",
              "                      -6.2743e-03, -5.0778e-03,  3.4149e-02,  3.6340e-02,  2.7881e-02,\n",
              "                       3.6172e-03,  3.0246e-02,  8.4189e-03, -1.6747e-02,  3.5521e-02,\n",
              "                      -3.5566e-02,  2.7212e-02,  2.0209e-02, -2.1225e-02, -6.7284e-03,\n",
              "                       1.5768e-03, -2.7493e-02,  1.5582e-02, -2.2259e-02, -1.0171e-02,\n",
              "                       2.4183e-02, -1.3894e-02, -2.8614e-02, -9.5263e-03, -2.4843e-02,\n",
              "                      -2.9907e-02,  2.3218e-02, -1.3489e-02,  9.0167e-03,  4.6100e-03,\n",
              "                      -2.1226e-02, -2.0543e-03,  3.5403e-03, -2.1230e-02,  1.0610e-02,\n",
              "                       1.1470e-02,  2.1425e-02,  6.6051e-03,  3.1634e-02, -1.6189e-02,\n",
              "                       1.7596e-02, -2.3299e-02,  1.8561e-02, -2.1480e-02,  8.1033e-03,\n",
              "                       2.3287e-02,  2.9820e-03, -1.0511e-02, -1.5846e-02, -1.3832e-03,\n",
              "                      -1.2121e-02, -2.3507e-02,  3.7100e-02,  2.7184e-02,  2.5831e-03,\n",
              "                      -2.4349e-02, -2.8887e-02,  3.1539e-02, -2.4141e-02,  2.5708e-02,\n",
              "                       4.7218e-02, -1.8819e-02,  3.8820e-02,  2.6195e-02,  4.0936e-02,\n",
              "                       2.4060e-03,  2.4304e-02,  9.5139e-03,  2.0530e-02,  2.6648e-03,\n",
              "                       1.5616e-02, -2.8082e-02,  8.1560e-03, -2.6059e-02,  3.1128e-02,\n",
              "                      -2.4680e-02,  3.0850e-02,  2.0044e-02, -3.1481e-02,  1.9547e-02,\n",
              "                       2.2997e-02, -4.3342e-03,  1.6485e-04,  5.3641e-03, -2.2474e-02,\n",
              "                       3.0339e-02, -1.2742e-02, -1.1572e-02,  1.8377e-02,  4.9532e-03,\n",
              "                       6.4849e-03,  1.3290e-04,  1.1082e-02,  3.3317e-02,  1.8833e-02,\n",
              "                      -2.2393e-02, -3.1893e-02, -8.7853e-03, -1.9024e-02,  3.4243e-02,\n",
              "                       2.6439e-03,  1.3850e-02,  5.2331e-03,  1.7964e-02, -1.7538e-02,\n",
              "                       4.8210e-02,  2.5725e-02,  1.8073e-02,  5.6446e-03, -2.7370e-02,\n",
              "                      -2.2622e-02, -3.5257e-03,  3.9959e-02, -9.9045e-03, -1.7630e-02,\n",
              "                       2.5171e-02, -3.2219e-03,  2.7890e-02,  2.2666e-02,  2.1028e-02,\n",
              "                      -3.1411e-02, -1.1101e-02,  1.8510e-02,  7.3073e-04, -1.1908e-02,\n",
              "                       1.8063e-02,  7.3442e-03,  4.3239e-02,  1.8605e-02, -2.9083e-02,\n",
              "                       1.7797e-02,  6.7325e-03, -1.7017e-02,  2.7484e-02, -1.8407e-02,\n",
              "                      -2.3702e-02,  1.5384e-02,  4.7827e-03, -5.8242e-04,  1.3292e-02,\n",
              "                      -2.5968e-02,  2.1593e-02,  1.8473e-02,  3.3182e-03, -1.4860e-02,\n",
              "                       6.6969e-04, -1.5534e-02,  2.0783e-02, -2.0051e-02, -1.5776e-02,\n",
              "                       1.4077e-02, -2.6073e-02, -3.6565e-03,  2.7462e-02, -2.9479e-03,\n",
              "                      -4.2699e-03,  1.1201e-02,  2.6750e-02, -8.7149e-03,  3.0392e-02,\n",
              "                      -7.9365e-03,  1.9700e-02, -1.5323e-02, -1.3949e-02,  6.4567e-03,\n",
              "                      -2.4507e-02, -2.5422e-02,  4.3075e-02,  2.7962e-02,  8.5813e-03,\n",
              "                       4.2920e-02,  3.8343e-02,  9.5501e-03, -1.6699e-02,  3.9864e-04,\n",
              "                       3.3092e-02,  1.2715e-02,  3.1673e-04, -3.1520e-02,  3.5843e-02,\n",
              "                      -2.2380e-02, -1.0607e-02,  8.4379e-03, -2.4232e-02,  2.1606e-02,\n",
              "                       1.1645e-02,  1.9034e-02], device='cuda:0')),\n",
              "             ('linear_relu_stack.2.weight',\n",
              "              tensor([[-0.0123,  0.0136, -0.0328,  ..., -0.0206, -0.0221,  0.0085],\n",
              "                      [ 0.0062, -0.0106,  0.0354,  ...,  0.0419,  0.0126,  0.0132],\n",
              "                      [-0.0213,  0.0023,  0.0006,  ..., -0.0389,  0.0267, -0.0014],\n",
              "                      ...,\n",
              "                      [-0.0240, -0.0132,  0.0482,  ..., -0.0032, -0.0275,  0.0337],\n",
              "                      [ 0.0053,  0.0394,  0.0043,  ...,  0.0439,  0.0085, -0.0264],\n",
              "                      [-0.0275,  0.0395,  0.0077,  ..., -0.0110, -0.0128,  0.0441]],\n",
              "                     device='cuda:0')),\n",
              "             ('linear_relu_stack.2.bias',\n",
              "              tensor([ 4.8819e-02, -1.9698e-02,  2.3491e-02, -4.5038e-02, -3.1163e-02,\n",
              "                      -2.1372e-02,  2.4936e-03,  1.5080e-02, -3.1902e-02, -2.7249e-03,\n",
              "                      -3.6460e-02,  4.7670e-02,  1.2400e-02,  6.2733e-03, -3.6741e-02,\n",
              "                       4.8102e-02,  2.9293e-02,  9.3112e-03, -3.2431e-02, -2.3947e-02,\n",
              "                      -3.7037e-02, -3.8078e-02,  5.0928e-03, -1.0393e-03, -5.3155e-02,\n",
              "                      -1.8959e-04, -5.3074e-03,  6.0939e-02,  6.5861e-03, -2.6562e-02,\n",
              "                      -3.2597e-02, -2.7676e-02,  4.0404e-02, -3.5567e-03, -7.3431e-03,\n",
              "                      -2.1310e-02, -3.0340e-03, -1.6728e-02,  1.3817e-02, -2.3388e-02,\n",
              "                       2.1465e-02, -4.2363e-02, -8.5856e-03, -3.6513e-02,  3.5578e-02,\n",
              "                      -2.9229e-02,  5.0480e-02, -6.0140e-03, -1.9908e-02, -2.8978e-02,\n",
              "                       4.6504e-02, -3.0127e-03,  2.7567e-03,  1.4503e-02,  2.2002e-02,\n",
              "                      -7.8660e-03,  2.3027e-02,  3.3224e-03,  3.2557e-02, -2.3615e-02,\n",
              "                       4.9218e-02, -2.6359e-02,  5.9008e-03, -1.3452e-02, -3.9763e-02,\n",
              "                       1.5398e-02,  6.5108e-02,  5.6282e-02, -3.5257e-02, -1.1944e-02,\n",
              "                      -2.1996e-02,  2.3135e-02,  1.0307e-03, -6.9969e-03,  9.4488e-03,\n",
              "                      -2.9638e-02, -9.1595e-03,  4.0863e-02, -2.2024e-02, -1.9891e-02,\n",
              "                      -3.2638e-03,  2.6919e-02, -3.5691e-02,  4.6670e-02, -4.5114e-02,\n",
              "                       3.0700e-02,  3.3351e-02,  6.3367e-02, -3.7220e-02,  3.4286e-02,\n",
              "                      -9.8371e-03,  6.1556e-03, -1.0025e-02,  2.8468e-02,  4.3465e-02,\n",
              "                       3.9974e-02, -3.8087e-02,  3.2294e-02,  2.5359e-02, -1.5235e-02,\n",
              "                       3.8398e-02,  4.3128e-02, -2.5064e-02,  4.0853e-02,  1.8899e-02,\n",
              "                      -8.1628e-03, -9.4373e-03, -8.6215e-03, -1.3850e-02, -8.7541e-03,\n",
              "                       3.2045e-02,  7.9110e-03,  2.5353e-03,  4.5408e-02,  4.3574e-02,\n",
              "                      -1.0334e-02,  3.7468e-02, -1.8102e-02,  1.6646e-02,  1.1781e-03,\n",
              "                      -3.9073e-02, -5.6025e-03, -1.7598e-02,  1.0839e-02, -3.5952e-02,\n",
              "                      -3.6357e-02,  5.1543e-02,  5.0645e-02,  1.9020e-02,  5.0238e-03,\n",
              "                       1.5057e-02, -1.3378e-02,  9.2390e-03,  3.6285e-02,  2.4847e-02,\n",
              "                       4.1137e-02,  2.0451e-03, -7.3902e-03,  2.9839e-02,  2.2594e-02,\n",
              "                      -3.3391e-02, -7.3379e-03,  4.7665e-02, -1.3545e-02,  3.7861e-02,\n",
              "                      -3.3831e-03,  6.2284e-02,  2.6373e-02,  3.1484e-02,  1.9132e-02,\n",
              "                       4.0683e-03,  1.8590e-02,  5.6348e-02,  3.4110e-02, -2.4533e-02,\n",
              "                      -1.9320e-02, -2.8207e-02,  6.1518e-04,  3.0284e-02, -1.5810e-02,\n",
              "                       2.9335e-02, -4.1759e-02,  2.3458e-02,  1.7582e-02, -4.4427e-02,\n",
              "                      -3.5087e-02,  4.7210e-02, -1.6149e-02,  3.1370e-02, -5.7484e-02,\n",
              "                       1.7782e-02,  2.7674e-02, -1.8541e-02, -1.2787e-02,  1.9878e-02,\n",
              "                      -9.8760e-03, -8.7617e-03, -3.2197e-02,  4.5984e-02, -5.9418e-03,\n",
              "                      -1.4270e-02, -2.9456e-02,  3.3973e-02,  3.6445e-03,  2.0524e-02,\n",
              "                       2.2770e-02,  2.9345e-02,  3.7552e-02,  1.2340e-02,  6.0774e-02,\n",
              "                      -8.8283e-03,  1.8406e-02,  5.0594e-02,  6.5507e-02, -1.0844e-02,\n",
              "                      -1.3461e-02,  4.0453e-02,  5.9556e-03,  1.6450e-03,  3.2420e-02,\n",
              "                      -3.3670e-02,  1.7507e-02, -2.8112e-02, -4.2185e-02, -1.9233e-02,\n",
              "                      -2.0111e-02,  5.9626e-02, -1.6891e-02, -4.1871e-02,  2.5078e-02,\n",
              "                       2.2483e-02,  2.1328e-02, -2.6536e-02, -1.9839e-02,  3.9566e-02,\n",
              "                       9.6393e-03,  4.4785e-02,  1.4971e-02,  1.9076e-02,  1.5535e-02,\n",
              "                       5.8615e-03,  4.3215e-02, -3.9244e-02, -1.8426e-03, -4.3126e-03,\n",
              "                       4.0350e-02, -4.4831e-02,  5.3767e-04,  1.7586e-02, -2.6528e-02,\n",
              "                      -3.3482e-03,  1.6565e-02,  9.0874e-03,  7.7063e-03, -2.1225e-02,\n",
              "                      -1.9578e-02, -4.3647e-02,  8.7330e-03, -1.3453e-02, -4.4281e-02,\n",
              "                      -1.7445e-02, -4.1208e-02,  1.9325e-02, -2.2390e-02, -2.5408e-03,\n",
              "                      -3.0906e-02, -3.3214e-02,  6.9880e-03,  4.6270e-02,  5.6365e-02,\n",
              "                       1.8814e-02, -3.9156e-02,  2.0051e-02, -3.0901e-02, -6.1390e-03,\n",
              "                      -3.8563e-02, -1.3054e-02,  4.6429e-02, -2.1866e-02,  1.8601e-02,\n",
              "                      -2.7082e-02, -2.6884e-02, -2.3944e-02, -1.0305e-02,  4.6494e-02,\n",
              "                      -2.2893e-02,  3.4738e-02,  3.4435e-02,  3.9741e-03, -7.0551e-03,\n",
              "                      -3.1866e-02,  4.2870e-02,  2.4789e-02, -2.5896e-02,  2.6402e-02,\n",
              "                      -2.1744e-02, -2.5861e-02, -5.4218e-03, -2.2315e-02, -6.6785e-03,\n",
              "                       5.6341e-02,  7.0286e-02, -3.9347e-02, -3.6189e-02,  2.9242e-02,\n",
              "                       4.4203e-02, -3.4646e-02, -4.0957e-02,  4.2454e-02,  2.6788e-02,\n",
              "                       2.2559e-02, -1.5029e-02, -1.7774e-02, -3.1300e-02,  2.7756e-02,\n",
              "                       4.5438e-02,  4.2994e-03, -2.2328e-02,  3.2644e-02, -2.0717e-02,\n",
              "                       2.5222e-02, -2.7344e-02, -3.6644e-02, -7.3873e-03, -4.8591e-02,\n",
              "                       9.3116e-03,  5.1704e-05, -2.3451e-02, -1.0982e-02,  4.1081e-02,\n",
              "                      -2.0822e-02, -1.8269e-02, -3.3965e-02, -1.6600e-02,  2.3640e-02,\n",
              "                       4.1555e-02, -8.6710e-03,  2.7541e-02,  1.9362e-02, -1.3186e-02,\n",
              "                       4.0319e-02, -3.5389e-03,  4.8006e-03, -1.2249e-02,  7.3807e-02,\n",
              "                      -5.2158e-03, -2.7825e-02,  7.5458e-03, -5.5214e-03, -3.1510e-03,\n",
              "                       5.5042e-02, -3.3570e-02,  1.9953e-02,  1.5306e-02, -1.2143e-02,\n",
              "                       4.9189e-02,  4.6858e-02,  3.9622e-02,  4.5531e-02,  7.7007e-03,\n",
              "                      -3.9787e-02,  5.3607e-02, -3.1621e-02,  4.8213e-02,  2.0700e-02,\n",
              "                      -2.6353e-02,  2.0213e-02,  3.5371e-02,  6.4088e-03,  3.0135e-02,\n",
              "                       6.2182e-02,  2.3150e-02,  9.1269e-03, -2.0262e-02,  3.6026e-02,\n",
              "                      -3.4560e-02, -1.6656e-02, -1.6402e-02,  1.9892e-02, -1.6410e-02,\n",
              "                       2.7393e-02,  2.2191e-03,  5.6323e-02,  3.4713e-02, -3.0525e-02,\n",
              "                       4.4001e-02, -2.6062e-02, -3.8245e-02,  1.1965e-02, -2.1671e-02,\n",
              "                       8.0963e-03,  9.8625e-03,  2.5532e-02, -1.4840e-02, -2.8001e-02,\n",
              "                       9.1682e-03, -2.5269e-02, -2.3736e-02, -3.2063e-02,  9.5553e-03,\n",
              "                       2.3537e-02,  2.3569e-03, -5.0262e-03,  2.2620e-02,  3.3601e-02,\n",
              "                      -1.2053e-02,  3.3750e-02, -1.6463e-02,  1.8143e-02,  3.5040e-02,\n",
              "                       2.8589e-03, -4.5487e-02,  1.6626e-04, -9.4673e-03,  1.6162e-02,\n",
              "                       1.3716e-02, -1.2317e-02,  1.9784e-02,  1.7696e-02,  2.4495e-02,\n",
              "                       1.6913e-02,  4.3262e-02,  2.0030e-02,  4.3039e-02, -1.6515e-02,\n",
              "                      -5.7917e-03,  3.9191e-02,  4.9949e-03,  7.8617e-03,  2.2726e-02,\n",
              "                      -1.0013e-02, -3.7293e-02,  3.3405e-02, -3.6428e-02,  4.0838e-02,\n",
              "                       2.0442e-02, -2.8368e-02,  7.6033e-03,  2.1119e-02, -3.8950e-02,\n",
              "                      -1.2592e-02, -7.2493e-03,  8.7892e-03, -3.2342e-02, -3.1537e-02,\n",
              "                       8.5305e-04,  3.2050e-02,  1.7716e-02,  1.8041e-02, -9.9035e-03,\n",
              "                       2.4696e-02,  1.2390e-03,  1.9917e-02, -1.6186e-02,  2.5599e-02,\n",
              "                      -4.3728e-02,  1.7176e-02, -1.0671e-02,  3.2221e-02,  4.1441e-02,\n",
              "                       6.1515e-02,  2.1352e-02,  4.5201e-02, -4.1623e-03,  2.9673e-02,\n",
              "                       8.5341e-03, -9.3155e-03,  4.5679e-03,  1.5612e-03,  3.0065e-02,\n",
              "                      -1.1411e-02,  1.5793e-03,  2.0085e-02,  3.4236e-02,  3.5128e-02,\n",
              "                      -2.0559e-02, -1.4540e-02,  4.4420e-02, -1.0000e-03,  4.5670e-02,\n",
              "                       3.3356e-02, -1.6476e-02, -3.6535e-02, -1.4556e-02,  3.4733e-02,\n",
              "                       4.0431e-02, -2.4907e-02,  4.1490e-03,  2.0935e-02, -2.6743e-02,\n",
              "                       5.1905e-02, -4.4812e-02,  3.2846e-02, -3.7313e-02, -2.9345e-02,\n",
              "                       3.7318e-03,  1.8615e-02,  1.2989e-02, -3.7242e-02,  1.5800e-02,\n",
              "                      -1.3657e-02, -1.7545e-02, -1.3237e-02,  3.1931e-02,  1.1927e-02,\n",
              "                      -5.7944e-03,  4.2472e-02,  1.8398e-02, -2.8799e-02, -1.5306e-02,\n",
              "                       4.0390e-04,  3.9240e-02,  4.4668e-03, -1.2067e-05, -2.8321e-02,\n",
              "                       5.6975e-03, -3.7972e-02, -3.1333e-02,  3.4826e-02,  1.9091e-02,\n",
              "                       2.1695e-02, -2.6345e-02,  1.9224e-02, -4.7192e-02,  5.5067e-02,\n",
              "                       2.9506e-02, -3.0334e-02, -2.7486e-02, -1.7416e-02,  4.3550e-02,\n",
              "                       5.6964e-02, -2.0879e-02], device='cuda:0')),\n",
              "             ('linear_relu_stack.4.weight',\n",
              "              tensor([[ 0.0752, -0.0291,  0.0578,  ...,  0.0753, -0.0457, -0.0593],\n",
              "                      [-0.0456, -0.0312,  0.1058,  ...,  0.0710,  0.0744,  0.0281],\n",
              "                      [ 0.0612,  0.0014, -0.0657,  ...,  0.0675,  0.0028,  0.0703],\n",
              "                      ...,\n",
              "                      [-0.0429, -0.0242, -0.0127,  ..., -0.0610,  0.0294,  0.0092],\n",
              "                      [ 0.0130,  0.0399,  0.0063,  ..., -0.0944, -0.0648,  0.0118],\n",
              "                      [-0.0317,  0.0011, -0.0152,  ..., -0.0860, -0.0251, -0.0368]],\n",
              "                     device='cuda:0')),\n",
              "             ('linear_relu_stack.4.bias',\n",
              "              tensor([-0.0134, -0.0166, -0.0578,  0.0362, -0.1159,  0.3153,  0.0487,  0.0897,\n",
              "                      -0.0720, -0.0711], device='cuda:0'))])"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for key, tensor in model.state_dict().items():\n",
        "    print(f\"层名称: {key} \\t 维度: {tensor.shape}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ePFPCLaNKPIu",
        "outputId": "c65631c1-45f2-40c1-da35-b8ce8fcda792"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "层名称: linear_relu_stack.0.weight \t 维度: torch.Size([512, 784])\n",
            "层名称: linear_relu_stack.0.bias \t 维度: torch.Size([512])\n",
            "层名称: linear_relu_stack.2.weight \t 维度: torch.Size([512, 512])\n",
            "层名称: linear_relu_stack.2.bias \t 维度: torch.Size([512])\n",
            "层名称: linear_relu_stack.4.weight \t 维度: torch.Size([10, 512])\n",
            "层名称: linear_relu_stack.4.bias \t 维度: torch.Size([10])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I0oh1kBt_8Qb",
        "outputId": "36a4375a-c307-49ac-ceb0-9092c89a460c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saved PyTorch Model State to model.pth\n"
          ]
        }
      ],
      "source": [
        "torch.save(model.state_dict(), \"model.pth\")\n",
        "print(\"Saved PyTorch Model State to model.pth\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IcBCYgfL_8Qc"
      },
      "source": [
        "Loading Models\n",
        "==============\n",
        "\n",
        "The process for loading a model includes re-creating the model structure\n",
        "and loading the state dictionary into it.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J6s8D4gC_8Qc",
        "outputId": "f389dc37-c170-4fa8-a203-ebb674072a90"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ],
      "source": [
        "model = NeuralNetwork().to(device)    # re-creating the model structure\n",
        "model.load_state_dict(torch.load(\"model.pth\", weights_only=True))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MTjZokI__8Qc"
      },
      "source": [
        "This model can now be used to make predictions.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eoXYRxKy_8Qc",
        "outputId": "3e1de97f-36ee-4dc1-a773-01e2328e846f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Predicted: \"Ankle boot\", Actual: \"Ankle boot\"\n"
          ]
        }
      ],
      "source": [
        "classes = [\n",
        "    \"T-shirt/top\",\n",
        "    \"Trouser\",\n",
        "    \"Pullover\",\n",
        "    \"Dress\",\n",
        "    \"Coat\",\n",
        "    \"Sandal\",\n",
        "    \"Shirt\",\n",
        "    \"Sneaker\",\n",
        "    \"Bag\",\n",
        "    \"Ankle boot\",\n",
        "]\n",
        "\n",
        "model.eval()\n",
        "x, y = test_data[0][0], test_data[0][1]\n",
        "with torch.no_grad():\n",
        "    x = x.to(device)\n",
        "    pred = model(x)\n",
        "    predicted, actual = classes[pred[0].argmax(0)], classes[y]    #只有一个维度\n",
        "    print(f'Predicted: \"{predicted}\", Actual: \"{actual}\"')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iL65Jk7E_8Qc"
      },
      "source": [
        "Read more about [Saving & Loading your\n",
        "model](saveloadrun_tutorial.html).\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}